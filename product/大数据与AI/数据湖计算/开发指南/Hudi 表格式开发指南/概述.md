Apache Hudi 是新一代流式数据湖平台，其最主要的特点是支持记录（Record）级别的插入更新（Upsert）和删除，同时还支持增量查询。
您可以在创建表、写入表和查询表中使用 Hudi 表格式。如果您在 DLC 上使用 Hudi 表格式遇到了问题，可以 [提交工单](https://console.cloud.tencent.com/workorder/category) 联系我们。

## 应用场景
- 近实时数据入湖
Hudi 支持插入、更新和删除数据的能力。相比其他传统的文件格式，Hudi 优化了数据写入过程中产生的小文件问题。
您可以基于 DLC Spark 或 Flink 实时摄取消息队列（Kafka 等）的日志数据至 Hudi  中，同时也支持实时同步数据库 Binlog 产生的变更数据。

- 增量数据处理
过去的增量处理往往将数据划分成小时粒度的分区，当属于此分区内的数据写入完成时，该分区就能对外提供相应的查询，这使数据的“新鲜程度”可以达到小时级别。但如果发生数据迟到的现象，唯一的补救措施是通过对整个分区的重新计算来保证正确性，这增加了整个系统的在计算和存储方面的性能开销。
Hudi 支持 Incremental Query 查询类型，您可以通过 DLC Spark Streaming 查询给定 COMMIT 后发生变更的数据，这降低了在计算资源方面的消耗，同时可以将数据的新鲜程度从小时级别提升到分钟级别，让数据在湖内不同层之间快速流转。

- 近实时数据分析
Hudi 通过将数据的更新时间缩短至几分钟，提供了一种面向实时分析更有效的方案。此外，借助于 DLC Presto和 SparkSQL 与 Hudi 的无缝集成和出色性能，您可以在无需任何额外配置的情况下，对更实时的数据进行更快的分析。

