模型仓库支持导入模型。列表页管理着不同的模型以及模型的不同版本；导入窗口支持从任务式建模直接导入模型以及从 COS 上传。

## 仓库列表

模型仓库列表中可查看模型及对应的版本，并对模型进行发布封装等操作，分为模型列表和优化模型列表，模型列表为客户上传上来的原始模型列表，优化模型列表为经过模型优化后保存的优化后的模型列表。
![](https://qcloudimg.tencent-cloud.cn/raw/a80859b20ab527d146c71ef33383c784.png)
- 支持对模型进行删除，单击**删除**按钮，此模型及对应的模型版本皆被删除。
- 支持对模型标签进行编辑，单击**编辑标签**按钮，可进行标签编辑。
- 支持对模型版本进行优化，在某一个模型版本点击**优化模型**，可跳转到模型优化新建任务页面进行优化。
- 支持对模型版本发布封装，在某一个模型版本单击**发布封装**，可跳转到模型服务新建服务页面进行模型服务发布。
- 支持对模型版本进行批量预测，在某一个模型版本单击**批量预测**，可跳转到批量预测新建任务页面进行批量预测。
- 支持对模型版本进行导入子版本，在某一个模型版本单击**导入模型**，可在当期模型版本目录下新增目录和子版本，目前仅支持对 Savedmodel 格式进行导入子版本，主要用于 tfserving 热更新使用。
- 支持重新上传模型相关文件，单击**上传文件**进行编辑模型文件，需要注意的是，重新编辑的模型相关文件需要在服务更新后才能生效。
- 支持查看关联服务组，单击**查看关联服务组**可查看模型版本关联的模型服务组以及状态；
- 支持查看关联跑批任务，单击**查看关联跑批任务**可查看模型版本关联的跑批任务以及状态；
- 支持下载 sdk 操作，单单击**下载 sdk** 按钮可打开 COSBrowser 进行下载。您需要先安装 [COSBrowser 客户端](https://cloud.tencent.com/document/product/436/11366#.E4.B8.8B.E8.BD.BD.E4.B8.8E.E5.AE.89.E8.A3.85)。
- 单击版本对应的**删除**按钮可删除对应的模型版本，需要注意的是，删除后不可恢复，已关联服务组并且服务在启动运行中的模型版本不支持删除，已关联批量预测任务并且任务在启动运行中的模型版本不支持删除。删除时可单击**查看关联服务组**或者**查看关联跑批任务**进行查看以确定是否要进行删除。

## 导入模型

进入**模型管理** > **模型仓库**，单击**导入模型** 按钮开始导入模型。



### 导入新模型
<img src="https://qcloudimg.tencent-cloud.cn/raw/0e1c9efaf556bf67e52a7df6408904f1.png" width="70%">

1. 按要求填入模型名称、自定义设置标签。（标签用于从不同维度对资源分类管理。用户需要先从控制台进行 [标签设置](https://console.cloud.tencent.com/tag/taglist)。对于标签设置您可以直接单击 **标签管理使用指南**查看。）
2. 从任务导入：
  - 选择训练任务名称。
  - 选择训练任务之后，然后选择对应的模型格式，如有任务对应的算法框架、模型来源路径、模型指标信息，信息将会被自动填充，模型指标填充的是最新上报指标。
  - 运行环境您可选择内置的，也可通过自定义选择的方式从容器镜像服务中拉取，优化后的模型目前仅支持内置运行环境运行。
  - 模型文件也会自动获取此任务的模型文件，您可选择剪切或者复制的方式对此模型文件进行处理，处理完成后将自动为您生成 COS 路径。
3. 从 COS 导入：
  - 选择模型格式，目前模型格式支持 Savedmodel、Frozen Graph、TorchScript、Detectron2、PyTroch 原生、PMML、ONNX、MMDetection、Hugging Face。
  - 输入您期望上传的模型对应的模型指标
  - 运行环境您可选择内置的，支持内置的运行环境对应上述支持的模型格式，也可通过自定义选择的方式从容器镜像服务中拉取。
  - 上传模型文件：需要您上传模型文件、推理代码和配置文件。推理脚本代码和服务 API 配置文件请分别命名为 config.json、model_service.py，具体请参考 [模型包规范](https://cloud.tencent.com/document/product/851/74145) 和 [推理脚本代码示例](https://cloud.tencent.com/document/product/851/74148)。
 

###  导入新版本
<img src="https://qcloudimg.tencent-cloud.cn/raw/4d972af83d964e26d331c3ba120b6fb1.png" width="70%">

1. 您可选择一个模型后导入这个模型的新版本，模型版本号将自动在历史最高版本号基础上+1，如V2、V3等。
2. 从任务导入：
	-	选择训练任务名称。
	-	选择训练任务之后，然后选择对应的模型格式，如有任务对应的算法框架、模型来源路径模型指标信息，信息将会被自动填充，模型指标填充的是最新上报指标。
	-	运行环境您可选择内置的，也可通过自定义选择的方式从容器镜像服务中拉取，优化后的模型目前仅支持内置运行环境运行
	-	模型文件也会自动获取此任务的模型文件，您可选择剪切或者复制的方式对此模型文件进行处理，处理完成后将自动为您生成 COS 路径。
3. 从 COS 导入：
	-	选择模型格式，目前模型格式支持 Savedmodel、Frozen Graph、TorchScript、Detectron2、PyTroch原生、PMML、ONNX、MMDetection、Hugging Face。
	-	输入您期望上传的模型对应的模型指标
	-	运行环境您可选择内置的，支持内置的运行环境对应上述支持的模型格式，也可通过自定义选择的方式从容器镜像服务中拉取。
	-	上传模型文件：需要您上传模型文件、推理脚本代码和服务 API 配置文件。推理代码和配置文件请分别命名为 config.json、model_service.py ，具体请参考 [模型包规范](https://cloud.tencent.com/document/product/851/74145) 和 [推理脚本代码示例](https://cloud.tencent.com/document/product/851/74148)。

完成以上配置后，单击**确定**按钮，模型便导入成功。

### 导入至当前版本
<img src="https://qcloudimg.tencent-cloud.cn/raw/b8fc92797fbe7e7a91f54a3fc48c3e97.png" width="70%">

1. 您可选择一个模型版本后导入这个模型版本的子版本，目前仅支持 Savedmodel 格式，主要用于 tfserving 热更新，您可以选择已经存在于模型列表里的 Savedmodel 格式模型和版本。
2. 从任务导入：
	- 选择 Savedmodel 格式模型的训练任务名称。
	- 选择训练任务之后，如有任务对应的算法框架、模型来源路径模型指标信息，信息将会被自动填充，模型指标填充的是最新上报指标。
	- 运行环境您可选择内置的，也可通过自定义选择的方式从容器镜像服务中拉取，优化后的模型目前仅支持内置运行环境运行
	- 模型导入目录为追加在该模型版本下的存储目录，该目录需要是模型版本主目录或者其子目录，可以选择在模型版本目录追加子版本的目录，平台会自动根据时间戳生成新目录，您可以根据需求选择是否勾选
	- 开启模型清理后将在模型版本对应的“模型保存路径”目录下寻找“model”目录，并根据模型数量保留上限的设定值，保留 model 目录里最新的N个模型
	- 模型文件也会自动获取此任务的模型文件，您可选择剪切或者复制的方式对此模型文件进行处理，处理完成后将自动为您生成 COS 路径
3. 从 COS 导入： 
	- 选择模型格式，目前模型格式支持 Savedmodel
	- 运行环境您可选择内置的，支持内置的运行环境对应上述支持的模型格式，也可通过自定义选择的方式从容器镜像服务中拉取
	- 模型来源路径，请选择您要导入的模型子版本对应的文件夹
	- 输入您期望上传的模型对应的模型指标
	- 模型导入目录为追加在该模型版本下的存储目录，该目录需要是模型版本主目录或者其子目录，可以选择在模型版本目录追加子版本的目录，平台会自动根据时间戳生成新目录，您可以根据需求选择是否勾选，会将您选中的模型来源路径夹下的所有文件复制或者粘贴到模型导入目录
	- 开启模型清理后将在模型版本对应的“模型保存路径”目录下寻找“model”目录，并根据模型数量保留上限的设定值，保留model目录里最新的N个模型
	- 会根据选择的模型移动方式将您选中的模型来源路径夹下的所有文件复制或者粘贴到模型导入目录
	
完成以上配置后，单击**确定**按钮，导入至现有版本便导入成功。


## 模型优化
原始模型列表的模型可在模型优化模块进行推理加速，请在参考 [模型优化](https://cloud.tencent.com/document/product/851/74676) 对模型进行推理加速。

## 发布封装
原始模型在原始模型列表对应的模型版本操作中，单击**发布封装**进行服务的发布封装，优化模型在原始模型列表对应的模型版本操作中，单击**发布封装**进行服务的发布封装，后续详细步骤请参考 [模型服务](https://cloud.tencent.com/document/product/851/74139) 对模型进行部署和使用。















